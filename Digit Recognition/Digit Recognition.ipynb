{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "2. [Imports](#Imports)\n",
    "3. [Data Preprocessing](#Data-Preprocessing)\n",
    "4. [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "5. [Data Processing](#Data-Processing)<br>\n",
    "5.1 [Normalization](#Normalization) <br>\n",
    "5.2 [Reshape](#Reshape) <br>\n",
    "5.3 [Categorization](#Categorization) <br>\n",
    "5.4 [Train Test Split](#Train-Test-Split) <br>\n",
    "6. [Data Augmentation](#Data-Augmentation)\n",
    "7. [Convolutional Neural Network](#Convolutional-Neural-Network)\n",
    "8. [Evaluate the Model](#Evaluate-the-Model)\n",
    "9. [Prediction](#Prediction)\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook is an attempt at solving a digit recognition problem. I built a Convolutional Neural Network (CNN) using Keras on top of a Tensorflow backend.\n",
    "\n",
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulfournier/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Activation, Conv2D, Dense, Dropout, Flatten, Input, MaxPool2D\n",
    "import keras.optimizers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel774</th>\n",
       "      <th>pixel775</th>\n",
       "      <th>pixel776</th>\n",
       "      <th>pixel777</th>\n",
       "      <th>pixel778</th>\n",
       "      <th>pixel779</th>\n",
       "      <th>pixel780</th>\n",
       "      <th>pixel781</th>\n",
       "      <th>pixel782</th>\n",
       "      <th>pixel783</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n",
       "0      1       0       0       0       0       0       0       0       0   \n",
       "1      0       0       0       0       0       0       0       0       0   \n",
       "2      1       0       0       0       0       0       0       0       0   \n",
       "3      4       0       0       0       0       0       0       0       0   \n",
       "4      0       0       0       0       0       0       0       0       0   \n",
       "\n",
       "   pixel8    ...     pixel774  pixel775  pixel776  pixel777  pixel778  \\\n",
       "0       0    ...            0         0         0         0         0   \n",
       "1       0    ...            0         0         0         0         0   \n",
       "2       0    ...            0         0         0         0         0   \n",
       "3       0    ...            0         0         0         0         0   \n",
       "4       0    ...            0         0         0         0         0   \n",
       "\n",
       "   pixel779  pixel780  pixel781  pixel782  pixel783  \n",
       "0         0         0         0         0         0  \n",
       "1         0         0         0         0         0  \n",
       "2         0         0         0         0         0  \n",
       "3         0         0         0         0         0  \n",
       "4         0         0         0         0         0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df = train_df.copy()\n",
    "processed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column represents the digit that the pixels represent and the columns pixel0:pixel783 are the pixels in a 28 by 28 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['label', 'pixel0', 'pixel1', 'pixel2', 'pixel3', 'pixel4', 'pixel5',\n",
       "       'pixel6', 'pixel7', 'pixel8',\n",
       "       ...\n",
       "       'pixel774', 'pixel775', 'pixel776', 'pixel777', 'pixel778', 'pixel779',\n",
       "       'pixel780', 'pixel781', 'pixel782', 'pixel783'],\n",
       "      dtype='object', length=785)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is already in integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label     int64\n",
       "pixel0    int64\n",
       "pixel1    int64\n",
       "pixel2    int64\n",
       "pixel3    int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_df.dtypes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "There seems to be a balanced number of each of the digits from 0 to 9 in the set. So there is no need to stratify the data before the train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "0    4132\n",
       "1    4684\n",
       "2    4177\n",
       "3    4351\n",
       "4    4072\n",
       "5    3795\n",
       "6    4137\n",
       "7    4401\n",
       "8    4063\n",
       "9    4188\n",
       "Name: pixel0, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.groupby(\"label\").count()[\"pixel0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data represents a pixel value on a scale of 0 to 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label         9\n",
       "pixel0        0\n",
       "pixel1        0\n",
       "pixel2        0\n",
       "pixel3        0\n",
       "pixel4        0\n",
       "pixel5        0\n",
       "pixel6        0\n",
       "pixel7        0\n",
       "pixel8        0\n",
       "pixel9        0\n",
       "pixel10       0\n",
       "pixel11       0\n",
       "pixel12     116\n",
       "pixel13     254\n",
       "pixel14     216\n",
       "pixel15       9\n",
       "pixel16       0\n",
       "pixel17       0\n",
       "pixel18       0\n",
       "pixel19       0\n",
       "pixel20       0\n",
       "pixel21       0\n",
       "pixel22       0\n",
       "pixel23       0\n",
       "pixel24       0\n",
       "pixel25       0\n",
       "pixel26       0\n",
       "pixel27       0\n",
       "pixel28       0\n",
       "           ... \n",
       "pixel754      0\n",
       "pixel755      0\n",
       "pixel756      0\n",
       "pixel757      0\n",
       "pixel758      0\n",
       "pixel759      0\n",
       "pixel760      0\n",
       "pixel761    177\n",
       "pixel762    231\n",
       "pixel763    253\n",
       "pixel764    254\n",
       "pixel765    254\n",
       "pixel766    255\n",
       "pixel767    255\n",
       "pixel768    255\n",
       "pixel769    255\n",
       "pixel770    255\n",
       "pixel771    255\n",
       "pixel772    255\n",
       "pixel773    255\n",
       "pixel774    254\n",
       "pixel775    254\n",
       "pixel776    253\n",
       "pixel777    253\n",
       "pixel778    254\n",
       "pixel779     62\n",
       "pixel780      0\n",
       "pixel781      0\n",
       "pixel782      0\n",
       "pixel783      0\n",
       "Length: 785, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Our Convolutional Neural Network will perform better when the pixel values are normalized. When an image has poor quality (like contrast due to glare) we will still be able to train our model confidently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_train = train_df[\"label\"]\n",
    "x_train = train_df.drop([\"label\"], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_train_normalized = normalize(x_train)\n",
    "test_df_normalized = normalize(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshape\n",
    "\n",
    "The data is now a Numpy Dataframe with each value representing a pixel intensity. We are going to build a CNN so we need to convert it to an actual image: 28 pixels by 28 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reshape image in 3 dimensions (height = 28px, width = 28px , canal = 1)\n",
    "x_train_normalized = x_train_normalized.reshape(-1,28,28,1)\n",
    "test_df_normalized = test_df_normalized.reshape(-1,28,28,1)\n",
    "\n",
    "g = plt.imshow(x_train_normalized[0][:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorization\n",
    "\n",
    "The activation function of the last layer of our CNN will be a softmax function (which will tell the level of certainty with witch the model is about the categorization), so we will one hot encode the categories to achieve better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = to_categorical(y_train, num_classes = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-Test Split\n",
    "\n",
    "Let us split the data into a training (90%) and testing set (10%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x_train_normalized, y_train, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation\n",
    "\n",
    "This step is made to generate a new set of data to train the model on from the given data. <br>\n",
    "Depending on the handwritting, the orientation of number will differ, we need our model to capture the different orientations so we will create additional images from original ones rotating them. In addition, some numbers can be recognized when flipped horizontally and/or vertically, like zero or eight, but some others could be confused with others, like a six and nine. To avoid deceiving the model, we will just zoom and shift on the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(\n",
    "        rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        zoom_range = 0.1, # Randomly zoom image \n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    ")\n",
    "\n",
    "datagen.fit(x_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Network\n",
    "\n",
    "Our CNN has the following architecture: two Conv2D layer it has size 32. The convolution is performed by a kernel (matrix) of 5 by 5. This choice of parameters is made to maximize the extraction of generic shapes in the layer. Then a MaxPool2D layer is added as well as a Dropout layer with probability 50% (so about half of the values from the previous layer will be dropped, this is done to avoid overfitting in our model). Then we add the same layers before finishing with a Flatten and a Dense Layer. The layers have a bigger number of unit as we go deeper. This is done to ensure that shapes captured in the deeper layers are simply combinations of the generic shapes captured in the shallow layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=(28,28,1))\n",
    "hidden = Conv2D(32, kernel_size=(5, 5), strides=(1, 1), activation='relu')(x)\n",
    "hidden = Conv2D(32, kernel_size=(5, 5), strides=(1, 1), activation='relu')(hidden)\n",
    "hidden = MaxPool2D(pool_size=(2, 2), strides=(2, 2))(hidden)\n",
    "hidden = Dropout(0.5)(hidden)\n",
    "hidden = Conv2D(64, kernel_size=(3, 3), strides=(1, 1), activation='relu')(hidden)\n",
    "hidden = Conv2D(64, kernel_size=(3, 3), strides=(1, 1), activation='relu')(hidden)\n",
    "hidden = MaxPool2D(pool_size=(2, 2), strides=(2, 2))(hidden)\n",
    "hidden = Dropout(0.5)(hidden)\n",
    "hidden = Flatten()(hidden)\n",
    "hidden = Dense(128, activation='relu')(hidden)\n",
    "y = Dense(10, activation='softmax')(hidden)\n",
    "\n",
    "model = Model(inputs=x, outputs=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the Adam optimizer as it has the best training cost reduction with more epochs. ([ref](https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/05/Comparison-of-Adam-to-Other-Optimization-Algorithms-Training-a-Multilayer-Perceptron.png)). I will train it with two callbacks: EarlyStopping to avoid overfitting and ReduceLROnPlateau to change the learning rate and improve learning when validation loss does not change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = keras.optimizers.SGD(lr=0.01, momentum=0., decay=0., nesterov=False)\n",
    "# optimizer = keras.optimizers.Adagrad(lr=0.01, epsilon=1e-6)\n",
    "# optimizer = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=1e-6)\n",
    "# optimizer = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=1e-6)\n",
    "optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', patience=2, verbose=1, factor=0.4, min_lr=0.000001)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=1, mode='auto')\n",
    "\n",
    "\n",
    "model.compile(loss=keras.losses.categorical_crossentropy, optimizer=optimizer, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "37800/37800 [==============================] - 9189s 243ms/step - loss: 0.0720 - acc: 0.9777 - val_loss: 0.0253 - val_acc: 0.9940\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(datagen.flow(x_train, y_train, batch_size=64),\n",
    "                              epochs = 1, validation_data = (x_test, y_test),\n",
    "                              verbose = 1, steps_per_epoch=x_train.shape[0],\n",
    "                              callbacks=[learning_rate_reduction, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the Model\n",
    "\n",
    "Let us look at where the model is wrong with the confusion matrix (we will see if patterns of errors are recurrent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7c73c16e9622>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Convert predictions classes to one hot vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0my_pred_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_pred\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;31m# Convert validation observations to one hot vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "# Look at confusion matrix \n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "# Predict the values from the validation dataset\n",
    "y_pred = model.predict(x_test)\n",
    "# Convert predictions classes to one hot vectors \n",
    "y_pred_classes = np.argmax(y_pred,axis = 1) \n",
    "# Convert validation observations to one hot vectors\n",
    "y_true = np.argmax(y_test,axis = 1) \n",
    "# compute the confusion matrix\n",
    "confusion_mtx = confusion_matrix(y_true, y_pred_classes) \n",
    "# plot the confusion matrix\n",
    "plot_confusion_matrix(confusion_mtx, classes = range(10)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some error results \n",
    "\n",
    "# Errors are difference between predicted labels and true labels\n",
    "errors = (y_pred_classes - y_true != 0)\n",
    "\n",
    "y_pred_classes_errors = y_pred_classes[errors]\n",
    "y_pred_errors = y_pred[errors]\n",
    "y_true_errors = y_true[errors]\n",
    "x_val_errors = x_test[errors]\n",
    "\n",
    "def display_errors(errors_index,img_errors,pred_errors, obs_errors):\n",
    "    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n",
    "    n = 0\n",
    "    nrows = 2\n",
    "    ncols = 3\n",
    "    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n",
    "    for row in range(nrows):\n",
    "        for col in range(ncols):\n",
    "            error = errors_index[n]\n",
    "            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n",
    "            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n",
    "            n += 1\n",
    "\n",
    "# Probabilities of the wrong predicted numbers\n",
    "y_pred_errors_prob = np.max(y_pred_errors,axis = 1)\n",
    "\n",
    "# Predicted probabilities of the true values in the error set\n",
    "true_prob_errors = np.diagonal(np.take(y_pred_errors, y_true_errors, axis=1))\n",
    "\n",
    "# Difference between the probability of the predicted label and the true label\n",
    "delta_pred_true_errors = y_pred_errors_prob - true_prob_errors\n",
    "\n",
    "# Sorted list of the delta prob errors\n",
    "sorted_dela_errors = np.argsort(delta_pred_true_errors)\n",
    "\n",
    "# Top 6 errors \n",
    "most_important_errors = sorted_dela_errors[-6:]\n",
    "\n",
    "# Show the top 6 errors\n",
    "display_errors(most_important_errors, x_val_errors, y_pred_classes_errors, y_true_errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(test_df_normalized)\n",
    "prediction = np.argmax(prediction, axis = 1)\n",
    "prediction = pd.Series(prediction, name=\"Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"), predicion],axis = 1)\n",
    "submission.to_csv(\"hello.csv\",index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
